# ğŸ˜ Ubuntuì—ì„œ Hadoop 3.2.4 ì„¤ì¹˜ & HA í´ëŸ¬ìŠ¤í„° êµ¬ì¶•

---

## ğŸ“Œ ê°œìš”
- Ubuntu í™˜ê²½ì—ì„œ **Hadoop 3.2.4 í´ëŸ¬ìŠ¤í„° ì„¤ì¹˜, ë…¸ë“œ ì„¤ì •, ë„¤ì„ë…¸ë“œ/ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì € HA êµ¬ì„±** ê°€ì´ë“œ
- í´ëŸ¬ìŠ¤í„° HA êµ¬ì„±ìœ¼ë¡œ **ë„¤ì„ë…¸ë“œ ë° ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì € ê°€ìš©ì„± í™•ë³´**

ğŸš€ **Ansibleë¡œ ìë™í™”ëœ í™˜ê²½ ì„¤ì • ì˜ˆì‹œ**ëŠ” ğŸ”— [`Ansible ë ˆí¬ì§€í† ë¦¬`](https://github.com/sy0218/Ansible-Multi-Server-Setup)ì—ì„œ í™•ì¸í•˜ì„¸ìš”!

---
<br>

## âš™ï¸ í´ëŸ¬ìŠ¤í„° ì„œë²„ êµ¬ì„±
| **í˜¸ìŠ¤íŠ¸** | **ì—­í• **              | **ë©”ëª¨ë¦¬** | **CPU** |
|------------|--------------------|------------|---------|
| `m1`      | NameNode + DataNode ğŸŸ¢ğŸ”µ | 20G        | 3       |
| `m2`      | NameNode + DataNode ğŸŸ¢ğŸ”µ | 20G        | 3       |
| `s1`      | DataNode ğŸ”µ           | 20G        | 3       |

---
<br>

## âš™ï¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export ZOOKEEPER_HOME=/application/zookeeper
export HADOOP_HOME=/application/hadoop
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_LOG_DIR=/logs/hadoop
export HADOOP_PID_DIR=/var/run/hadoop/hdfs
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"
export HIVE_HOME=/application/hive
export HIVE_AUX_JARS_PATH=$HIVE_HOME/aux

export PATH=$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$HIVE_AUX_JARS_PATH/bin:$ZOOKEEPER_HOME/bin:$PATH

# ì ìš©
source ~/.bashrc
```

---
<br>

## âš™ï¸ Hadoop ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
```bash
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz
tar -xzvf hadoop-3.2.4.tar.gz
ln -s /application/hadoop-3.2.4 /application/hadoop
```

---
<br>

## âš™ï¸ Hadoop ì„¤ì • íŒŒì¼
### ğŸ”¹ core-site.xml
> í•˜ë‘¡ ê³µí†µ ì„¤ì •

> HDFS ê¸°ë³¸ ì„¤ì •ê³¼ HA í´ëŸ¬ìŠ¤í„° ë„¤ì„ì„œë¹„ìŠ¤, ì£¼í‚¤í¼ ì—°ê²° ì •ë³´ ë“±

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```xml
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://job-cluster</value>
                <description>ë„¤ì„ë…¸ë“œ HA êµ¬ì„± ì‹œ ì‚¬ìš©í•  í´ëŸ¬ìŠ¤í„° ë…¼ë¦¬ì  ì´ë¦„</description>
        </property>

        <property>
                <name>hadoop.http.staticuser.user</name>
                <value>root</value>
                <description>í´ëŸ¬ìŠ¤í„° ê¸°ë³¸ ì‚¬ìš©ì</description>
        </property>

        <property>
                <name>hadoop.tmp.dir</name>
                <value>file:///hdfs_tmp/hadoop-${user.name}</value>
                <description>í•˜ë‘¡ ì‘ì—…ê´€ë ¨ ì„ì‹œ ë””ë ‰í„°ë¦¬</description>
        </property>

        <property>
                <name>ha.zookeeper.quorum</name>
                <value>m1:2181,m2:2181,s1:2181</value>
                <description>ì£¼í‚¤í¼ ë…¸ë“œ</description>
        </property>
</configuration>
```
</details>

---

### ğŸ”¹ hdfs-site.xml
> HDFS ê´€ë ¨ ì„¤ì •

> HDFS ë°ì´í„° ë””ë ‰í„°ë¦¬, HA ë„¤ì„ë…¸ë“œ, ì €ë„ë…¸ë“œ, ìë™ ì¥ì•  ì „í™˜ ë“±

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```xml
<configuration>
        <property>
                <name>dfs.nameservices</name>
                <value>job-cluster</value>
                <description>ë„¤ì„ì„œë¹„ìŠ¤ ì´ë¦„</description>
        </property>

        <property>
                <name>dfs.ha.namenodes.job-cluster</name>
                <value>nn1,nn2</value>
                <description>ë„¤ì„ë…¸ë“œ</description>
        </property>

        <property>
                <name>dfs.namenode.rpc-address.job-cluster.nn1</name>
                <value>m1:9000</value>
                <description>nn1ì˜ RPC í¬íŠ¸</description>
        </property>

        <property>
                <name>dfs.namenode.rpc-address.job-cluster.nn2</name>
                <value>m2:9000</value>
                <description>nn2ì˜ RPC í¬íŠ¸</description>
        </property>

        <property>
                <name>dfs.namenode.http-address.job-cluster.nn1</name>
                <value>m1:50070</value>
                <description>nn1ì˜ UI</description>
        </property>

        <property>
                <name>dfs.namenode.http-address.job-cluster.nn2</name>
                <value>m2:50070</value>
                <description>nn2ì˜ UI</description>
        </property>

        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:///job/hdfs/nn</value>
                <description>í•˜ë‘¡ ë„¤ì„ë…¸ë“œ ë””ë ‰í† ë¦¬</description>
        </property>

        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:///data1,file:///data2</value>
                <description>í•˜ë‘¡ ë°ì´í„° ë””ë ‰í† ë¦¬</description>
        </property>

        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://m1:8485;m2:8485;s1:8485/job-cluster</value>
                <description>ê³ ê°€ìš©ì„±ì„ ìœ„í•œ ì €ë„ë…¸ë“œ ì§€ì •</description>
        </property>

        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/job/hdfs/jn</value>
                <description>ê³ ê°€ìš©ì„±ì„ ìœ„í•œ ì €ë„ë…¸ë“œ ë””ë ‰í† ë¦¬</description>
        </property>

        <property>
                <name>dfs.client.failover.proxy.provider.job-cluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>

        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>shell(/bin/true)</value>
        </property>

        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/root/.ssh/id_rsa</value>
        </property>

        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>
</configuration>
```
</details>

---

### ğŸ”¹ yarn-site.xml
> yarn ê´€ë ¨ ì„¤ì •

> ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì €, ë…¸ë“œ ë§¤ë‹ˆì €, HA, ë©”ëª¨ë¦¬/CPU ì œí•œ, FairScheduler ì„¤ì •

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>NodeManagerì—ì„œ MapReduce shuffle ì„œë¹„ìŠ¤ ì‹¤í–‰</description>
    </property>

    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        <description>ShuffleHandler í´ë˜ìŠ¤ ì§€ì •</description>
    </property>

    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
        <description>ê°€ìƒë©”ëª¨ë¦¬ ê²€ì‚¬ ë¹„í™œì„±í™” (ì‘ì€ í´ëŸ¬ìŠ¤í„°ì—ì„œ ê¶Œì¥)</description>
    </property>

    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
        <description>ë¬¼ë¦¬ë©”ëª¨ë¦¬ ê²€ì‚¬ ë¹„í™œì„±í™”</description>
    </property>

    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>10240</value>
        <description>NodeManagerê°€ ê´€ë¦¬í•  ì´ ë©”ëª¨ë¦¬ (20G ì„œë²„ì—ì„œ 10G ì‚¬ìš©)</description>
    </property>

    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>3</value>
        <description>NodeManagerê°€ ê´€ë¦¬í•  CPU ì½”ì–´ ìˆ˜ (ì„œë²„ ì½”ì–´ ìˆ˜ ê¸°ì¤€)</description>
    </property>

    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
        <description>YARNì—ì„œ ResourceRequest ìµœì†Œ ë©”ëª¨ë¦¬ ë‹¨ìœ„ (1G)</description>
    </property>

    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>5120</value>
        <description>YARNì—ì„œ ResourceRequest ìµœëŒ€ ë©”ëª¨ë¦¬ ë‹¨ìœ„ (5G)</description>
    </property>

    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
        <description>Fair Scheduler ì‚¬ìš©</description>
    </property>

    <property>
        <name>yarn.scheduler.fair.allocation.file</name>
        <value>/application/hadoop/etc/hadoop/fair-scheduler.xml</value>
        <description>Fair Scheduler Pool ì •ì˜ íŒŒì¼ ê²½ë¡œ</description>
    </property>

    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
        <description>ResourceManager HA í™œì„±í™”</description>
    </property>

    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
        <description>HA RM ID ë¦¬ìŠ¤íŠ¸</description>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>m1</value>
        <description>RM1 í˜¸ìŠ¤íŠ¸ ì´ë¦„</description>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>m2</value>
        <description>RM2 í˜¸ìŠ¤íŠ¸ ì´ë¦„</description>
    </property>

    <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>m1:8088</value>
        <description>RM1 ì›¹ UI ì£¼ì†Œ</description>
    </property>

    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>m2:8088</value>
        <description>RM2 ì›¹ UI ì£¼ì†Œ</description>
    </property>

    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>job-cluster</value>
        <description>í´ëŸ¬ìŠ¤í„° ID</description>
    </property>

    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>m1:2181,m2:2181,s1:2181</value>
        <description>YARN HA Zookeeper ì£¼ì†Œ (RM ìƒíƒœ ì €ì¥)</description>
    </property>

    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
        <description>RM ìƒíƒœ ë³µêµ¬ í´ë˜ìŠ¤ (ZK ì‚¬ìš©)</description>
    </property>

    <property>
        <name>yarn.client.failover-proxy-provider</name>
        <value>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</value>
        <description>í´ë¼ì´ì–¸íŠ¸ RM HA failover í”„ë¡œë°”ì´ë”</description>
    </property>

    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
        <description>RM HA ìƒíƒœ ë³µêµ¬ í™œì„±í™”</description>
    </property>
</configuration>
```
</details>

---

### ğŸ”¹ yarn-env.sh
> yarn í™˜ê²½ë³€ìˆ˜ ì„¤ì •

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```bash
export YARN_RESOURCEMANAGER_HEAPSIZE=10240
```
</details>

---

### ğŸ”¹ hadoop-env.sh
> Hadoop í™˜ê²½ë³€ìˆ˜ ì„¤ì •

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HEAPSIZE_MAX=10240
export HADOOP_HEAPSIZE_MIN=1024
```
</details>

---

### ğŸ”¹ workers
> ë…¸ë“œë§¤ë‹ˆì € + ë°ì´í„°ë…¸ë“œ ì‹¤í–‰ í˜¸ìŠ¤íŠ¸ ëª©ë¡

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```text
m1
m2
s1
```
</details>

---

### ğŸ”¹ fair-scheduler.xml
> YARN FairScheduler ì •ì±… ì„¤ì •

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```xml
<?xml version="1.0"?>
<allocations>
  <user name="root">
    <maxRunningApps>5</maxRunningApps>
  </user>
</allocations>
```
</details>

---

### ğŸ”¹ hadoop-config.sh
> Hadoop ì‚¬ìš©ì í™˜ê²½ ë³€ìˆ˜

<details>
<summary>â–¶ï¸ í´ë¦­í•˜ì—¬ ë³´ê¸°</summary>

```bash
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
export HDFS_ZKFC_USER=root
export HDFS_JOURNALNODE_USER=root
```
</details>

---
<br>

## âš™ï¸ Hadoop HA í´ëŸ¬ìŠ¤í„° ê¸°ë™
### ğŸ”¹ Master (m1)
```bash
hdfs zkfc -formatZK       # ZK Failover Controllerìš© ZooKeeper ì´ˆê¸°í™”
start-dfs.sh               # HDFS(NameNode, DataNode) ì‹œì‘
hdfs namenode -format      # NameNode ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” (ì²« ì‹¤í–‰ ì‹œ)
stop-dfs.sh                # HDFS ì ì‹œ ì¤‘ì§€ (ì´ˆê¸°í™” í›„ ì¬ì‹œì‘ ì¤€ë¹„)
start-all.sh               # HDFS + YARN ì „ì²´ í´ëŸ¬ìŠ¤í„° ì‹œì‘
```
---
### ğŸ”¹ Master2 (m2)
```bash
hdfs namenode -bootstrapStandby  # Standby NameNode ì´ˆê¸° ë™ê¸°í™”
hadoop-daemon.sh start namenode  # Standby NameNode ì‹œì‘
yarn-daemon.sh start resourcemanager # YARN ResourceManager ì‹œì‘
```

---
<br>

## ğŸ” HA ìƒíƒœ í™•ì¸
### ğŸ”¹ ë„¤ì„ë…¸ë“œ
```bash
hdfs haadmin -getServiceState nn1  # active
hdfs haadmin -getServiceState nn2  # standby
```
---
### ğŸ”¹ ë¦¬ì†ŒìŠ¤ë§¤ë‹ˆì €
```bash
yarn rmadmin -getServiceState rm1  # standby
yarn rmadmin -getServiceState rm2  # active
```

---
<br>

## ğŸ” ìµœì¢… í™•ì¸
```bash
hdfs dfsadmin -report
```
```nginx
# ì˜ˆì‹œ ì¶œë ¥
Configured Capacity: 31178293248 (29.04 GB)
Present Capacity: 29466746880 (27.44 GB)
DFS Remaining: 29466599424 (27.44 GB)
DFS Used: 147456 (144 KB)
DFS Used%: 0.00%
Live datanodes (3):
  Name: 192.168.122.63:9866 (m1)
  Name: 192.168.122.64:9866 (m2)
  Name: 192.168.122.65:9866 (s1)
```

---
<br>

## âœ… ì°¸ê³  ì‚¬í•­
- **HA êµ¬ì„± í•„ìˆ˜**: NameNodeì™€ ResourceManagerë¥¼ HAë¡œ êµ¬ì„±í•´ì•¼ í´ëŸ¬ìŠ¤í„° ê°€ìš©ì„± í™•ë³´ ê°€ëŠ¥.
- **ë…¸ë“œë³„ ì—­í•  êµ¬ë¶„**:  
  - `m1`, `m2` â†’ NameNode + DataNode  
  - `s1` â†’ DataNode ì „ìš©  
- **HDFS ë° YARN ë””ë ‰í„°ë¦¬**:  
  - `dfs.namenode.name.dir`, `dfs.datanode.data.dir`, `dfs.journalnode.edits.dir` ê²½ë¡œëŠ” **ì¡´ì¬í•˜ê³  ì“°ê¸° ê¶Œí•œ** í•„ìš”.
- **ZooKeeper ì˜ì¡´ì„±**:  
  - HA NameNodeì™€ RM HA ì„¤ì • ì‹œ **ZooKeeper quorum** (`ha.zookeeper.quorum` / `yarn.resourcemanager.zk-address`) ì •ìƒ ë™ì‘ í•„ìš”.
- **Hadoop ì‚¬ìš©ì í™˜ê²½**:  
  - `HDFS_NAMENODE_USER`, `HDFS_DATANODE_USER`, `YARN_RESOURCEMANAGER_USER` ë“± ëª¨ë“  ë°ëª¬ ì‚¬ìš©ìëŠ” **root ë˜ëŠ” ê¶Œí•œ ìˆëŠ” ê³„ì •**ìœ¼ë¡œ ì§€ì •.
- **í´ëŸ¬ìŠ¤í„° ì‹œì‘ ìˆœì„œ**:  
  1. ZooKeeper ì‹œì‘  
  2. NameNode/JournalNode í¬ë§· ë° ì‹œì‘  
  3. DataNode, ResourceManager, NodeManager ì‹œì‘  
- **Standby NameNode ì´ˆê¸°í™”**: `hdfs namenode -bootstrapStandby` ë°˜ë“œì‹œ ìˆ˜í–‰.
- **YARN ë¦¬ì†ŒìŠ¤ ì œí•œ**:  
  - NodeManager ë©”ëª¨ë¦¬/CPU ì„¤ì • (`yarn.nodemanager.resource.memory-mb`, `yarn.nodemanager.resource.cpu-vcores`) ì ì ˆíˆ ì¡°ì •.
- **FairScheduler ì‚¬ìš© ì‹œ**: `fair-scheduler.xml`ì—ì„œ ì‚¬ìš©ìë³„ ìµœëŒ€ ì‹¤í–‰ ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ ì œí•œ ê°€ëŠ¥.
- **ì„œë¹„ìŠ¤ ìë™í™”**:  
  - `systemd` ì„œë¹„ìŠ¤ ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ë“±ë¡ ì‹œ ì„œë²„ ì¬ë¶€íŒ… í›„ **ìë™ ì‹œì‘ ë° ê´€ë¦¬** ê°€ëŠ¥.
- **HA ìƒíƒœ í™•ì¸**:  
  - NameNode: `hdfs haadmin -getServiceState <nn>`  
  - ResourceManager: `yarn rmadmin -getServiceState <rm>`
- **ìµœì¢… í™•ì¸**:  
  - `hdfs dfsadmin -report` ë¡œ í´ëŸ¬ìŠ¤í„° ìš©ëŸ‰, ì‚¬ìš©ëŸ‰, DataNode ìƒíƒœ ì ê²€.
- **ì£¼ì˜**:  
  - HDFS í¬ë§·(`hdfs namenode -format`)ì€ **ì²« ì‹¤í–‰ ì‹œë§Œ** ìˆ˜í–‰, ì´ë¯¸ ìš´ì˜ ì¤‘ì¸ ë°ì´í„°ëŠ” ì‚­ì œë¨.  
  - í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘ ì‹œ NameNode/RM active/standby ìƒíƒœ í™•ì¸ í•„ìˆ˜.  
  - HA failover, fencing, ZK ì„¤ì • ì •í™•íˆ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ìë™ ì¥ì•  ì „í™˜ ì‹¤íŒ¨ ê°€ëŠ¥.
---
